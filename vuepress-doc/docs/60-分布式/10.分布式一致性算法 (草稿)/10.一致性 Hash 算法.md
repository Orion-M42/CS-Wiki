---
permalink: /pages/consistency-hash/
title: 一致性 Hash 算法
date: 2022-10-08 10:30:31
---


> 参考：
>
> - [redis系列之——一致性hash算法](https://zhuanlan.zhihu.com/p/179266232)
> - [Redis一致性hash算法](https://www.jianshu.com/p/ae4139bdbbc4?u_atoken=6debf5c7-cd7e-4335-aee7-13a8a0cda4eb&u_asession=01OJXoAelymdwDrul_ECojd23aZQH3yv1Q4MTaxEFsnZkXFRedD_c4UCCvCh8DYxjoX0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K_aJvZrYOrzvafQFYsk-7rNPpcarp92QKzyJKyYjREPlmBkFo3NEHBv0PZUm6pbxQU&u_asig=05tHaCXnN_m0k5f1xXCnJ12SOX47txjQEBxvGb3sJJjYoEKbz-IQFzq9K10F0W-D9aq0gHyygE-iQ6vL0pb_zkl5RcP9N9R4O0mJxaOnWSa2k0go4I5bXlGdF8lhU0ootk5dijv-urOP3XAjzBO04APEVEbjXKlh0pf165lGvsif_9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzScUZG8XURcP0pPBJ0GCNkrz_pPf1PlEnFbUXyJx9JkHWPRPQyB_SKrj-61LB_f61u3h9VXwMyh6PgyDIVSG1W8vsEEviB5JZsvT3XX072JAnnfigo6gvybXhopmdXmQQEgXI8AV-so0jrscDk4wG21-LrPxhc7FnS7-li6R3rogmWspDxyAEEo4kbsryBKb9Q&u_aref=D5R%2FlvLGi7tj86CMVpHHxNK9AYM%3D)

一致性哈希用到的地方很多，特别是中间件里面，比如 Dubbo 的负载均衡也有一种策略是一致性哈希策略，使用的就是虚拟节点实现的。**Redis 集群中也用到了相关思想，但是没有直接使用用它，如上所述是根据实际情况改进了一下，引入了哈希槽（slot）的概念**。哈希槽的本质和一致性哈希算法非常相似，不同点就是对于哈希空间的定义

## 简单哈希

`hash(key) % length`

问题在于，集群扩容（添加机器）的时候，length 发生变化，大部分数据都会由于重新 Hash 导致被分配到其他机器上。这种操作会导致服务在一定的时间不可用，而且每次扩缩容都会存在这个问题。换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据，导致**缓存雪崩**

## 一致性哈希

一致性hash算法主要应用于分布式存储系统中，可以有效地解决分布式存储结构下普通余数 Hash 算法带来的伸缩性差的问题，可以保证在动态增加和删除节点的情况下尽量有多的请求命中原来的机器节点。

一致性 Hash 算法也是使用取模的方法，只是，简单哈希的取模法是对服务器的数量进行取模，而一致性Hash算法是对 $2^ {32}-1$ 取模 `hash(key) % (2^32 - 1)`

简单来说，一致性Hash算法将整个Hash值组织成一个虚拟的圆环，值空间为 $[0 , 2^{32}-1]$（32 位无符号整型数），整个哈希环如下：

<img src="https://pic1.zhimg.com/80/v2-c92c94797e46f1bf24affcbfe3aff4e8_1440w.jpg" alt="img" style="zoom: 67%;" />

整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^ 32-1，我们把这个由 2^32 个点组成的圆环称为 Hash 环。

下一步将各个服务器使用 Hash 进行一个哈希，**具体可以选择服务器的主机名（考虑到ip变动，不要使用ip）作为 key 进行哈希**，这样每台机器就能确定其在哈希环上的位置，这里假设将三个服务器节点的IP地址哈希后在环空间的位置如下：

<img src="https://pic1.zhimg.com/80/v2-158b0f541a3c09806a0e2180011be924_1440w.jpg" alt="img" style="zoom:67%;" />

下面插入三条 key-value 数据，具体该放到哪个服务器节点上呢？

**将数据 key 使用相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置。将数据从所在位置顺时针找第一台遇到的服务器节点，这个节点就是该 key 存储的服务器！**

例如我们有a、b、c三个key，经过哈希计算后，在环空间上的位置如下：key-a存储在node1，key-b存储在node2，key-c存储在node3。

<img src="https://pic1.zhimg.com/80/v2-adc9e2114f3c426cec275e35b11f7380_1440w.jpg" alt="img" style="zoom:67%;" />

## 容错性和可扩展性

现假设**Node 2**不幸宕机，可以看到此时对象key-a和key-c不会受到影响，**只有key-b被顺时针重定位到Node 3**。

一般的，**在一致性Hash算法中，如果一台服务器不可用 or 新增一台服务器，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器，如下图中Node 2与Node 1之间的数据，图中受影响的是key-2）之间数据，其它不会受到影响**。

同样的，如果集群中新增一个node 4，受影响的数据是node 1和node 4之间的数据，其他的数据是不受影响的。

<img src="https://pic2.zhimg.com/80/v2-5bf04a2870c9a777c24a0fa9e3bf1b39_1440w.jpg" alt="img" style="zoom:67%;" />

综上所述，**一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性**。

## 数据倾斜

**一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜**（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，此时必然造成大量数据集中到Node 2上，而只有极少量会定位到Node 1上。其环分布如下：

<img src="https://pic4.zhimg.com/80/v2-b12da674c0c91c0e73c2f0ab6f7f13ef_1440w.jpg" alt="img" style="zoom:67%;" />

为了解决数据倾斜问题，一致性 Hash 算法引入了**虚拟节点**机制：**对每一个服务器节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点**。具体做法可以在主机名的后面增加编号来实现。

例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node 1#1”、“Node 1#2”、“Node 1#3”、“Node 2#1”、“Node 2#2”、“Node 2#3”的哈希值，于是形成六个虚拟节点：

<img src="https://pic2.zhimg.com/80/v2-5af08ddb812550d3284cd54c71c7fa19_1440w.jpg" alt="img" style="zoom:67%;" />

同时数据定位算法不变，**只是多了一步虚拟节点到实际节点的映射**，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

> 以下参考：https://www.pdai.tech/md/algorithm/alg-domain-distribute-x-consistency-hash.html

下面来说下宕机之后的情况：

![](https://cs-wiki.oss-cn-shanghai.aliyuncs.com/img/image-20221008114823180.png)

假设Real1机器宕机，则会发生一下情况。

- 原先存储在虚拟节点V100上的k1数据将迁移到V301上，也就意味着迁移到了Real3机器上。
- 原先存储再虚拟节点V101上的k4数据将迁移到V200上，也就意味着迁移到了Real2机器上。

从这里可以看出来，某个节点宕机之后，存储及流量压力并没有全部转移到某台机器上，而是分散到了多台节点上。解决了节点宕机可能存在的雪崩问题。

总结来说就是，**当物理节点多的时候，虚拟节点多，雪崩的可能性就越小**。
